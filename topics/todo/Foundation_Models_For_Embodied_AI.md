# Foundation Models for Embodied AI

## 1. Affordance

* **[ICCV 21]** Where2Act: From Pixels to Actions for Articulated 3D Objects, [website](https://cs.stanford.edu/~kaichun/where2act/)

* **[CVPR 23]** Affordance Diffusion: Synthesizing Hand-Object Interactions, [website](https://judyye.github.io/affordiffusion-www/)

* **[ICCV 23]** AffordPose: A Large-scale Dataset of Hand-Object Interactions with Affordance-driven Hand Pose, [website](https://affordpose.github.io/)

* **[arXiv 24]** Robo-ABC: Affordance Generalization Beyond Categories via Semantic Correspondence for Robot Manipulation, [arXiv](https://arxiv.org/abs/2401.07487)

* **[CVPR 22]** Joint Hand Motion and Interaction Hotspots Prediction from Egocentric Videos, [website](https://stevenlsw.github.io/hoi-forecast/)

* **[CVPR 19]** ContactGrasp: Functional Multi-finger Grasp Synthesis from Contact, [website](https://contactdb.cc.gatech.edu/contactgrasp.html)

* **[3DV 20]** Grasping Field: Learning Implicit Representations for Human Grasps, [PDF](https://arxiv.org/pdf/2008.04451)

* **[IROS 24(Oral)]** ManipVQA: Injecting Robotic Affordance and Physically Grounded Information into Multi-Modal Large Language Models, [website](https://github.com/SiyuanHuang95/ManipVQA)

* **[CVPR 23]** VRB: Affordances from Human Videos as a Versatile Representation for Robotics, [website](https://robo-affordances.github.io/)

* **[arXiv 24]** Articulated Object Manipulation with Coarse-to-fine Affordance for Mitigating the Effect of Point Cloud Noise: https://arxiv.org/pdf/2402.18699

* **[arXiv 24]** General Flow as Foundation Affordance for Scalable Robot Learning, [website](https://general-flow.github.io/)

* **[ICCV 23]** ContactGen: Generative Contact Modeling for Grasp Generation, [website](https://stevenlsw.github.io/contactgen/)

* **[CVPR 20(Oral)]** GanHand: Predicting Human Grasp Affordances in Multi-Object Scenes / YCB-Affordance Dataset, [website](https://enriccorona.github.io/ganhand/)

* **[ICCV 21(Oral)]** Hand-Object Contact Consistency Reasoning for Human Grasps Generation, [website](https://hwjiang1510.github.io/GraspTTA/)

* **[ICRA 21]** Learning Dexterous Grasping with Object-Centric Visual Affordances, [website](https://vision.cs.utexas.edu/projects/graff-dexterous-affordance-grasp/)


* **[arXiv 24]** Interactive Humanoid: Online Full-Body Motion Reaction Synthesis with Social Affordance Canonicalization and Forecasting, [website](https://yunzeliu.github.io/iHuman/)

* **[ECCV 22]** AdaAfford: Learning to Adapt Manipulation Affordance for 3D Articulated Objects via Few-shot Interactions, [website](https://hyperplane-lab.github.io/AdaAfford/)

* **[ICCV 23]** Learning Foresightful Dense Visual Affordance for Deformable Object Manipulation, [website](https://hyperplane-lab.github.io/DeformableAffordance/)

* **[ICLR 22]** VAT-Mart: Learning Visual Action Trajectory Proposals for Manipulating 3D ARTiculated Objects, [website](https://hyperplane-lab.github.io/vat-mart/)

* **[arXiv 24]** PreAfford: Universal Affordance-Based Pre-Grasping for Diverse Objects and Environments, [arXiv](https://air-discover.github.io/PreAfford/)

* **[NIPS 23]** Where2Explore: Few-shot Affordance Learning for Unseen Novel Categories of Articulated Objects, [arXiv](https://arxiv.org/abs/2309.07473)

* **[ICCV 19]** Grounded Human-Object Interaction Hotspots From Video, [website](https://vision.cs.utexas.edu/projects/interaction-hotspots/)

* **[CVPR 20]** Ego-Topo: Environment Affordances From Egocentric Video, [website](https://vision.cs.utexas.edu/projects/ego-topo/)

* **[IROS 24(Oral)]** ManiFoundation Model for General-Purpose Robotic Manipulation of Contact Synthesis with Arbitrary Objects and Robots, [website](https://manifoundationmodel.github.io/)

### Dataset for Affordance
* **[arXiv 24]** DexMimicGen: Automated Data Generation for Bimanual Dexterous Manipulation via Imitation Learning, [website](https://dexmimicgen.github.io/)

* **[CVPR 19 (Oral, Best Paper Finalist)]** ContactDB: Analyzing and Predicting Grasp Contact via Thermal Imaging, [website](https://arxiv.org/pdf/1904.06830)/[Code](https://github.com/samarth-robo/contactdb_prediction)

* **[ECCV 20]** ContactPose: A Dataset of Grasps with Object Contact and Hand Pose, [website](https://contactpose.cc.gatech.edu/)

* **[CVPR 19]** ObMan Dataset synthetic Object Manipulation, [website](https://www.di.ens.fr/willow/research/obman/data/)

* **[ICCV 23]** AffordPose: A Large-scale Dataset of Hand-Object Interactions with Affordance-driven Hand Pose, [website](https://affordpose.github.io/)

* **[ICCV 23]** ContactGen: Generative Contact Modeling for Grasp Generation, [website](https://stevenlsw.github.io/contactgen/)

* **[ICCV 19]** FreiHAND: A Dataset for Markerless Capture of Hand Pose and Shape from Single RGB Images, [website](https://lmb.informatik.uni-freiburg.de/projects/freihand/)

* **[CVPR 20(Oral)]** GanHand: Predicting Human Grasp Affordances in Multi-Object Scenes / YCB-Affordance Dataset, [website](https://enriccorona.github.io/ganhand/)

* **[CVPR 21]** DexYCB: A Benchmark for Capturing Hand Grasping of Objects, [website](https://dex-ycb.github.io/)

* **[ECCV 20]** GRAB: A Dataset of Whole-Body Human Grasping of Objects, [Website](https://grab.is.tue.mpg.de/)

* **[CVPR 24]** TACO: Benchmarking Generalizable Bimanual Tool-ACtion-Object Understanding, [website](https://taco2024.github.io/)

* **[TPAMI 20]** The EPIC-KITCHENS Dataset: Collection, Challenges and Baselines, [arXiv](https://arxiv.org/abs/2005.00343)

* **[CVPR 21]** 3D AffordanceNet: A Benchmark for Visual Object Affordance Understanding, [website](https://andlollipopde.github.io/3D-AffordanceNet/#/)

* **[CVPR 22]** Human Hands As Probes for Interactive Object Understanding, [website](https://s-gupta.github.io/hands-as-probes/)

## 2. Generation & Reconstruction
* **[CVPR 19]** Learning joint reconstruction of hands and manipulated objects, [website](https://hassony2.github.io/obman)


## 3. Tracking & Estimation

* **[CVPR 20(Oral)]** Understanding Human Hands in Contact at Internet Scale, [website](https://fouheylab.eecs.umich.edu/~dandans/projects/100DOH/)

* **[CVPR 24]** TAPIR: Towards Spatial Intelligence via Point Tracking, [website](https://deepmind-tapir.github.io/blogpost.html)

* **[arXiv 24]** HOT3D: Hand and Object Tracking in 3D from Egocentric Multi-View Videos, [website](https://facebookresearch.github.io/hot3d/)

* **[CVPR 20]** HOnnotate: A method for 3D Annotation of Hand and Object Poses, [website](https://www.tugraz.at/index.php?id=40231)

## 4. Correspondance

* **[NIPS 23]** Emergent Correspondence from Image Diffusion, [website](https://diffusionfeatures.github.io/)

## 5. World Model

* **[post 24]** Genie 2: A large-scale foundation world model, [website](https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/)

## 6. Vision Foundation Model
* **[ICCV 23(Oral)]** SigLIP: Sigmoid Loss for Language Image Pre-Training, [website](https://github.com/google-research/big_vision)