<h1 align="center">
	Manipulation
</h1>

## Dexterous Manipulation

* **[arXiv 25]** Dexterity Gen: Foundation Controller for Unprecedented Dexterity, [website](https://zhaohengyin.github.io/dexteritygen/)

## Vision Language Action (VLA)

* **[arXiv 25]** EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos, [website](https://rchalyang.github.io/EgoVLA/)

* **[RSS 23]** Octo: An Open-Source Generalist Robot Policy, [website](https://octo-models.github.io/)

* **[arXiv 22]** RT-1: Robotics Transformer for Real-World Control at Scale, [arXiv](https://arxiv.org/abs/2212.06817)

* **[arXiv 23]** RT-2: Vision-Language-Action Models, [website](https://robotics-transformer2.github.io/)

* **[ICLR 24 (Spotlight)]** RT-Trajectory: Robotic Task Generalization via Hindsight Trajectory Sketches, [website](https://rt-trajectory.github.io/)

* **[arXiv 24]** OpenVLA: An Open-Source Vision-Language-Action Model, [website](https://openvla.github.io/)

* **[arXiv 25]** OpenVLA-oft: Fine-Tuning Vision-Language-Action Models:
Optimizing Speed and Success, [website](https://openvla-oft.github.io/)

* **[RAL 25]** TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation, [website](https://tiny-vla.github.io/)

* **[arXiv 24]** TraceVLA: Visual Trace Prompting Enhances Spatial-Temporal Awareness for Generalist Robotic Policies, [arXiv](https://arxiv.org/abs/2412.10345)

* **[arXiv 25]** π0: A Vision-Language-Action Flow Model for
General Robot Control, [pdf](https://www.physicalintelligence.company/download/pi0.pdf)

* **[arXiv 25]** π0.5: a Vision-Language-Action Model with Open-World Generalization, [arXiv](https://arxiv.org/abs/2504.16054)

* **[arXiv 25]** ChatVLA: Unified Multimodal Understanding and Robot Control with Vision-Language-Action Model, [website](https://chatvla.github.io/)

* **[arXiv 25]** ChatVLA-2: Vision-Language-Action Model with Open-World Embodied Reasoning from Pretrained Knowledge, [website](https://chatvla-2.github.io/)

* **[ICLR 25]** RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation, [website](https://rdt-robotics.github.io/rdt-robotics/)

* **[arXiv 25]** A0: An Affordance-Aware Hierarchical Model for General Robotic Manipulation, [website](https://a-embodied.github.io/A0/)

* **[arXiv 25]** SpatialVLA: Exploring Spatial Representations for Visual-Language-Action Model, [website](https://spatialvla.github.io/)

* **[arXiv 25]** UniVLA: Learning to Act Anywhere with Task-centric Latent Actions, [PDF](https://arxiv.org/pdf/2505.06111)

* **[CVPR 25]** UniAct: Universal Actions for Enhanced Embodied Foundation Models, [website](https://2toinf.github.io/UniAct/)

* **[arXiv 25]** DexVLA: Vision-Language Model with Plug-In Diffusion Expert for General Robot Control, [website](https://dex-vla.github.io/)

* **[arXiv 25]** GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data, [arXiv](https://arxiv.org/abs/2505.03233)

https://unified-video-action-model.github.io/

* **[ICLR 25]** Latent Action Pretraining from videos, [arXiv](https://arxiv.org/pdf/2410.11758)

## Grasping

* **[arXiv 25]** GraspGen: A Diffusion-based Framework for 6-DOF Grasping, [website](https://graspgen.github.io/)

## Cross-Embodiment

* **[arXiv 25]** AnyBody: A Benchmark Suite for Cross-Embodiment Manipulation, [website](https://princeton-vl.github.io/anybody/)

* **[CoRL 23]** PolyBot: Training One Policy Across Robots While Embracing Variability, [website](https://sites.google.com/view/polybot-multirobot)